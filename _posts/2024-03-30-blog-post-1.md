---
title: 'Understanding Knowledge Distillation'
date: 2024-03-30
permalink: /posts/2024/03/30/
---
Knowledge distillation (KD) is a method used for training small, compact, data-efficient models. It was first proposed by Hinton et al. [1] in 2015 and soon evolved into various forms. In the rest of the article, I will first introduce standard KD as well as its variants from an academic view. Afterwards, I will explain them in a more intuitive way.
